{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "2VOXnVIljLgm",
        "cEb4nO2jhBNW",
        "Rc1dEggTyUWK",
        "3gBv1ToeeHiZ",
        "iNpnvVOmmvi5",
        "kG7LoSQnu2B7",
        "Riyb4-SGzVAK",
        "6nIR0ip6DVhx",
        "oKPBP6--J9D5",
        "pepp_gZieB8D"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdz7A5lBzl1LGCyVHQjg/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RochaCesar26/Algoritmos-de-regresi-n-Ejemplos---API-Sklearn/blob/main/Algoritmos_de_regresi%C3%B3n_(Ejemplos)_API_Sklearn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Regression."
      ],
      "metadata": {
        "id": "2VOXnVIljLgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1\n",
        "Empecemos con un ejemplo simple, en el siguiente código se espera obtener el error cuadrático medio entre las predicciones y los valores reales en el conjunto de prueba."
      ],
      "metadata": {
        "id": "mD595u5pjvny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializar el modelo de Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSoi0ryDliUh",
        "outputId": "a9252d96-0069-4a4e-a611-ce6cfa0967a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1234.752982660588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2\n",
        "En el siguiente ejemplo, se utilizará el conjunto de datos de diabetes de Scikit-learn, que es otro conjunto de datos comúnmente utilizado en ejemplos, al igual que añade el uso de matplotlib.pyplot, que se utilizara para mostrar de manera grafica los resultados."
      ],
      "metadata": {
        "id": "PW1Mpi-asouH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las librerías necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Inicializar el modelo de Gradient Boosting Regression\n",
        "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio:\", mse)\n",
        "# Graficar las predicciones vs. los valores reales\n",
        "plt.scatter(y_test, y_pred)\n",
        "plt.xlabel(\"Valores reales\")\n",
        "plt.ylabel(\"Predicciones\")\n",
        "plt.title(\"Predicciones vs. Valores Reales\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N3JjemZss37H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3\n",
        "Continuemos con un tercer ejemplo con un poco más de complejidad."
      ],
      "metadata": {
        "id": "6jxPt-sfw89v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de viviendas de California\n",
        "california_housing = fetch_california_housing()\n",
        "X, y= california_housing.data, california_housing.target\n",
        "\n",
        "# Convertir los datos a un DataFrame de pandas\n",
        "df = pd.DataFrame(data= np.c_[california_housing['data'], california_housing['target']],\n",
        "                     columns= california_housing['feature_names'] + ['target'])\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Inicializar y ajustar el modelo Gradient Boosting Regression\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error Cuadrático Medio:\", mse)"
      ],
      "metadata": {
        "id": "PJCWOXryxNff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El código anterior utiliza el conjunto de datos de precios de viviendas de California. Este conjunto de datos contiene características sobre el precio medio de las viviendas en varios distritos de California. Utilizaremos Gradient Boosting Regression para predecir el precio medio de la vivienda basándonos en estas características."
      ],
      "metadata": {
        "id": "TO7Wkbiaxbau"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4\n",
        "El siguiente ejemplo será un poco más complejo, ya que se utiliza una validación cruzada para ajustar los hiperparámetros(más favorables) del modelo de Gradient Boosting Regression en un conjunto de datos."
      ],
      "metadata": {
        "id": "NcXJap-Q0CQs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir los hiperparámetros a ajustar\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "\n",
        "# Crear el modelo de regresión de aumento de gradiente\n",
        "gb_regressor = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
        "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Mejores hiperparámetros:\", best_params)\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_gb_regressor = grid_search.best_estimator_\n",
        "\n",
        "# Predecir en el conjunto de prueba utilizando el mejor modelo\n",
        "y_pred = best_gb_regressor.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio:\", mse)"
      ],
      "metadata": {
        "id": "ffBdbUCJ0Rxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Elastic Net Regression"
      ],
      "metadata": {
        "id": "cEb4nO2jhBNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1\n",
        "En este ejemplo vemos una introducción simple al uso del modelo de regresión Elastic Net."
      ],
      "metadata": {
        "id": "hlAZbUm-iSUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el modelo Elastic Net\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "elastic_net.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = elastic_net.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio:\", mse)"
      ],
      "metadata": {
        "id": "i03NrE57irUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2\n",
        "En este caso, utilizaremos el conjunto de datos de diabetes de scikit-learn al igual que matplotlib.pyplot para imprimir de manera grafica los datos."
      ],
      "metadata": {
        "id": "wGipJ5Cbkb5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = datasets.load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el modelo de Elastic Net Regression\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "\n",
        "# Entrenar el modelo\n",
        "elastic_net.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = elastic_net.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error Cuadrático Medio:\", mse)\n",
        "\n",
        "# Visualizar los coeficientes del modelo\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(elastic_net.coef_, color='skyblue', linewidth=2, marker='o', markersize=7)\n",
        "plt.title(\"Coeficientes del modelo Elastic Net\")\n",
        "plt.xlabel(\"Índice del coeficiente\")\n",
        "plt.ylabel(\"Valor del coeficiente\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bc--0syMkl4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3\n"
      ],
      "metadata": {
        "id": "irrfw7fjmBIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de precios de viviendas en California\n",
        "california_housing = fetch_california_housing()\n",
        "# Convertir el conjunto de datos a un DataFrame de pandas\n",
        "df = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
        "df['target'] = california_housing.target\n",
        "# Dividir los datos en características (X) y etiquetas (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Crear y entrenar el modelo Elastic Net Regression\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X_train_scaled, y_train)\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = elastic_net.predict(X_test_scaled)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error Cuadrático Medio:\", mse)\n",
        "# Visualizar los coeficientes del modelo\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(elastic_net.coef_, color='skyblue', linewidth=2, marker='o', markersize=7)\n",
        "plt.title(\"Coeficientes del modelo Elastic Net\")\n",
        "plt.xlabel(\"Índice del coeficiente\")\n",
        "plt.ylabel(\"Valor del coeficiente\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "89NyYrV8mq4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4\n",
        "Veamos un último ejemplo utilizando un conjunto de datos local."
      ],
      "metadata": {
        "id": "m2sSkqkHpA6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar los datos de precios de viviendas en Melbourne\n",
        "df = pd.read_csv('DatosMelbourne.csv')\n",
        "\n",
        "# Eliminar filas con valores faltantes en la variable objetivo\n",
        "df = df.dropna(subset=['Price'])\n",
        "# Seleccionar características y variable objetivo\n",
        "X = df[['Rooms', 'Distance', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt']]\n",
        "y = df['Price']\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Normalizar las características\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Crear y entrenar el modelo de Elastic Net Regression\n",
        "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
        "elastic_net.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "y_pred = elastic_net.predict(X_test_scaled)\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error Cuadrático Medio:\", mse)\n",
        "# Visualizar los coeficientes del modelo\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(elastic_net.coef_, color='skyblue', linewidth=2, marker='o', markersize=7)\n",
        "plt.title(\"Coeficientes del modelo Elastic Net\")\n",
        "plt.xlabel(\"Índice del coeficiente\")\n",
        "plt.ylabel(\"Valor del coeficiente\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yLLLX9aKsoNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stochastic Gradient Descent Regression."
      ],
      "metadata": {
        "id": "Rc1dEggTyUWK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "o7kQhT2NW8yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "X, y = make_regression(n_samples=1000, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear y entrenar el modelo de regresión SGD\n",
        "sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = sgd_regressor.predict(X_test)\n",
        "\n",
        "# Calcular el error (RMSE en este caso)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n"
      ],
      "metadata": {
        "id": "LCYFalnUXATe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2\n"
      ],
      "metadata": {
        "id": "Mgnpt4jbb4Nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo utilizaremos el conjunto de datos de diabetes de Scikit-learn, que contiene información sobre la progresión de la diabetes en 442 pacientes, así como 10 variables predictoras que pueden ser utilizadas para predecir la progresión de la enfermedad.\n",
        "Este ejemplo es similar al anterior, con el único cambio notorios es que cargamos un conjunto de datos en lugar de generarlos."
      ],
      "metadata": {
        "id": "Xs5qW0rEcLAJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crear y entrenar el modelo de regresión SGD\n",
        "sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_regressor.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = sgd_regressor.predict(X_test_scaled)\n",
        "\n",
        "# Calcular el error (RMSE en este caso)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVihgj0Ub_ik",
        "outputId": "2ffef2eb-414b-4ddd-f750-e09db1c3a7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error: 53.70027976126501\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "pGqfX5IOcasc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo, subiremos un poco la complejidad, aquí generamos datos de manera aleatoria y luego ajustaremos un modelo de regresión utilizando SGD. Una vez importadas las librerías empezamos a explicar paso a paso."
      ],
      "metadata": {
        "id": "_P_xhaHYcnzM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "np.random.seed(0)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test = X[:80], X[80:]\n",
        "y_train, y_test = y[:80], y[80:]\n",
        "\n",
        "# Crear y entrenar el modelo de regresión SGD\n",
        "sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_regressor.fit(X_train, y_train)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = sgd_regressor.predict(X_test)\n",
        "# Calcular el error (RMSE en este caso)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "print(\"Root Mean Squared Error:\", rmse)\n",
        "\n",
        "# Graficar los datos y las predicciones del modelo\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X_test, y_test, color='blue', label='Datos reales')\n",
        "plt.plot(X_test, y_pred, color='red', label='Predicciones')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Predicciones del modelo de regresión SGD')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GhXnk9JDctho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "dErXadAbd2QT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este ejemplo es similar al anterior, pero ahora utilizaremos un archivo local para realizar la regresión, y de recordatorio, asegúrense tener el archivo guardado en el mismo directorio de donde se ejecute el código."
      ],
      "metadata": {
        "id": "qN8Jmhyqd-5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar datos desde un archivo CSV\n",
        "data = pd.read_csv('diamonds.csv')\n",
        "\n",
        "# Dividir los datos en características (X) y variable objetivo (y)\n",
        "X = data.iloc[:, :-1].values  # Todas las columnas excepto la última\n",
        "y = data.iloc[:, -1].values   # Última columna\n",
        "\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear y entrenar el modelo de regresión SGD\n",
        "sgd_regressor = SGDRegressor(max_iter=1000, tol=1e-3, random_state=42)\n",
        "sgd_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = sgd_regressor.predict(X_test)\n",
        "\n",
        "# Calcular el error (RMSE en este caso)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "print(\"Root Mean Squared Error:\", rmse)"
      ],
      "metadata": {
        "id": "YR5Zi4B1eDkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Machine"
      ],
      "metadata": {
        "id": "3gBv1ToeeHiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "05JkP133e65z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo utilizaremos el conjunto de datos Iris, es un conjunto de datos muy conocido y disponible en scikit-learn. Lo que hace el código es cargar los datos, dividirlos en conjuntos de entrenamiento y prueba, escalar las características para normalizarlas, inicializar el clasificador SVM con un kernel lineal, entrenar los datos de entrenamiento y luego predice las clases para el conjunto de prueba. Finalmente, calcula la precisión del modelo utilizando las etiquetas verdaderas y las etiquetas predichas. Recorrámoslo a detalle:"
      ],
      "metadata": {
        "id": "xh3ojq_9frvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Cargar el conjunto de datos Iris (ejemplo de conjunto de datos)\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Escalar las características para normalizarlas\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Inicializar el clasificador SVM\n",
        "classifier = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# Entrenar el clasificador SVM\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predecir las clases para el conjunto de prueba\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calcular la precisión del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo SVM:\", accuracy)"
      ],
      "metadata": {
        "id": "aTPtCWb0fnjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2\n",
        "Veamos otro ejemplo básico de cómo implementar un Support Vector Machine (SVM) en Python utilizando scikit-learn. En este ejemplo, vamos a utilizar el conjunto de datos Breast Cancer Wisconsin.\n",
        "\n"
      ],
      "metadata": {
        "id": "Dowe45dihBNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Cargar el conjunto de datos Breast Cancer Wisconsin\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "# Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "# Escalar las características para normalizarlas\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "# Inicializar el clasificador SVM con un kernel lineal\n",
        "classifier = SVC(kernel='linear', random_state=42)\n",
        "# Entrenar el clasificador SVM\n",
        "classifier.fit(X_train, y_train)\n",
        "# Predecir las clases para el conjunto de prueba\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Calcular la precisión del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo SVM:\", accuracy)"
      ],
      "metadata": {
        "id": "HP_-68HPhTqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3\n"
      ],
      "metadata": {
        "id": "88FkxwdGiI1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo se busca mostrar la precisión del modelo SVM y un informe detallado de la clasificación en el conjunto de prueba. Además, se visualiza las fronteras de decisión del clasificador SVM en un gráfico bidimensional, lo que permite una comprensión visual de cómo el modelo divide el espacio de características para la clasificación."
      ],
      "metadata": {
        "id": "aPG0BnADiaM-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paso 1: Generar datos de clasificación binaria\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
        "# Paso 2: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Paso 3: Escalar las características para normalizarlas\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "# Paso 4: Inicializar y entrenar el clasificador SVM\n",
        "classifier = SVC(kernel='linear')\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Paso 5: Predecir las clases para el conjunto de prueba\n",
        "y_pred = classifier.predict(X_test)\n",
        "# Paso 6: Evaluar el rendimiento del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo SVM:\", accuracy)\n",
        "\n",
        "# Informe detallado de la clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "#Visualizar las fronteras de decisión (solo para dos características)\n",
        "def plot_decision_boundary(X, y, classifier):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
        "    ax = plt.gca()\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "\n",
        "    xx, yy = np.meshgrid(np.linspace(xlim[0], xlim[1], 50),\n",
        "                         np.linspace(ylim[0], ylim[1], 50))\n",
        "    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contour(xx, yy, Z, colors='k', levels=[-1, 0, 1], alpha=0.5,\n",
        "                linestyles=['--', '-', '--'])\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title('Decision Boundary of SVM Classifier')\n",
        "    plt.show()\n",
        "\n",
        "# Visualizar las fronteras de decisión (solo para dos características)\n",
        "X_train_subset = X_train[:, :2]\n",
        "classifier.fit(X_train_subset, y_train)\n",
        "plot_decision_boundary(X_train_subset, y_train, classifier)"
      ],
      "metadata": {
        "id": "-e-a1CvVicc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "XwC4XbZBkusD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente ejemplo es un poco más complejo, ya que involucra un conjunto de datos real y la optimización de parámetros utilizando búsqueda de hiperparámetros:"
      ],
      "metadata": {
        "id": "IvsN2moDkyvR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Paso 1: Cargar el conjunto de datos de cáncer de mama\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "# Paso 2: Dividir el conjunto de datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Paso 3: Escalar las características para normalizarlas\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "# Paso 4: Inicializar el clasificador SVM\n",
        "svm = SVC()\n",
        "\n",
        "# Paso 5: Definir los parámetros a optimizar y realizar la búsqueda de hiperparámetros\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'gamma': [1, 0.1, 0.01, 0.001],\n",
        "              'kernel': ['rbf', 'linear', 'poly']}\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "# Mejores parámetros encontrados\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Mejores parámetros encontrados:\", best_params)\n",
        "\n",
        "# Paso 6: Entrenar el clasificador SVM con los mejores parámetros encontrados\n",
        "best_svm = SVC(**best_params)\n",
        "best_svm.fit(X_train_scaled, y_train)\n",
        "# Paso 7: Predecir las clases para el conjunto de prueba\n",
        "y_pred = best_svm.predict(X_test_scaled)\n",
        "\n",
        "# Paso 8: Evaluar el rendimiento del modelo\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Precisión del modelo SVM:\", accuracy)\n",
        "# Informe detallado de la clasificación\n",
        "print(\"Informe de clasificación:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "jwtsohFik2vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Ridge Regression."
      ],
      "metadata": {
        "id": "iNpnvVOmmvi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "HapfWZ5ZqxlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "# Datos de entrada y salida\n",
        "X = [[0], [1], [2], [3], [4]]\n",
        "y = [0, 1, 2, 3, 4]\n",
        "\n",
        "# Crear el modelo\n",
        "model = BayesianRidge()\n",
        "# Entrenar el modelo\n",
        "model.fit(X, y)\n",
        "\n",
        "# Hacer predicciones\n",
        "predictions = model.predict([[5]])\n",
        "print(\"Predicción para x = 5:\", predictions[0])"
      ],
      "metadata": {
        "id": "NDYBbwSTqcBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2\n",
        "En este ejemplo vamos a utilizar el conjunto de datos de diabetes de la biblioteca Scikit-learn.\n",
        "\n"
      ],
      "metadata": {
        "id": "zOAbVM7YqsY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "# Dividir los datos en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalizar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Crear y entrenar el modelo de regresión de la bahía de la cresta\n",
        "model = BayesianRidge()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "predictions = model.predict(X_test_scaled)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, predictions)\n",
        "print(\"Error cuadrático medio:\", mse)\n"
      ],
      "metadata": {
        "id": "Zj5znzD5q_-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3\n",
        "En este ejemplo incluye una visualización de los datos y las predicciones realizadas por el modelo de regresión de la bahía de la cresta:"
      ],
      "metadata": {
        "id": "5aQURKEJrP9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importamos las bibliotcas\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "\n",
        "# Generamos datos de ejemplo\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1) * 10\n",
        "true_slope = 2\n",
        "true_intercept = 3\n",
        "y = true_slope * X.squeeze() + true_intercept + np.random.randn(100) * 2  # Relación lineal con ruido\n",
        "\n",
        "# Creamos el modelo de regresión de la bahía de la cresta\n",
        "model = BayesianRidge()\n",
        "# Entrenamos el modelo\n",
        "model.fit(X, y)\n",
        "\n",
        "# Hacemos predicciones para visualizar la línea de regresión\n",
        "X_range = np.linspace(0, 10, 100).reshape(-1, 1)\n",
        "y_pred, y_std = model.predict(X_range, return_std=True)\n",
        "\n",
        "# Visualizamos los datos y la línea de regresión\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(X, y, color='blue', label='Datos de entrenamiento')\n",
        "plt.plot(X_range, y_pred, color='red', label='Predicción')\n",
        "plt.fill_between(X_range.squeeze(), y_pred - y_std, y_pred + y_std, color='orange', alpha=0.3, label='Incertidumbre')\n",
        "plt.title('Regresión de la Bahía de la Cresta')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kjOkfcLirV-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "RCSs2aCOspr8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente ejemplo, generaremos datos sintéticos con una relación lineal y un poco de ruido gaussiano. Luego, dividimos los datos en conjuntos de entrenamiento y prueba, ajustaremos el modelo de regresión bayesiana a los datos de entrenamiento y evalúa su rendimiento en el conjunto de prueba utilizando el error cuadrático medio (MSE) y el coeficiente de determinación (R^2). Finalmente, traza un gráfico que muestra las predicciones del modelo versus los valores reales en el conjunto de prueba."
      ],
      "metadata": {
        "id": "APzvhG6Xs1GX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Paso 2: Generar datos sintéticos\n",
        "np.random.seed(42)\n",
        "X = np.linspace(0, 10, 100)\n",
        "y = 2 * X + np.random.normal(0, 1, 100)  # Relación lineal con ruido gaussiano\n",
        "\n",
        "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Paso 4: Crear y ajustar el modelo de regresión bayesiana\n",
        "bayesian_regressor = BayesianRidge()\n",
        "bayesian_regressor.fit(X_train.reshape(-1, 1), y_train)\n",
        "\n",
        "# Paso 5: Evaluar el rendimiento del modelo\n",
        "y_pred = bayesian_regressor.predict(X_test.reshape(-1, 1))\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "print(\"R^2 Score:\", r2)\n",
        "\n",
        "# Paso 6: Visualizar los resultados\n",
        "plt.scatter(X_test, y_test, color='black', label='Datos reales')\n",
        "plt.plot(X_test, y_pred, color='blue', linewidth=3, label='Predicciones')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Bayesian Ridge Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BYTihNVIs3Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CatBoost Regressor"
      ],
      "metadata": {
        "id": "kG7LoSQnu2B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1\n"
      ],
      "metadata": {
        "id": "c-vckvhOwoXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este primer ejemplo utilizando CatBoost Regressor, servirá para predecir los precios de las casas basándonos en características como el tamaño y el número de habitaciones."
      ],
      "metadata": {
        "id": "M4C310oJw4VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalación\n",
        "!pip install catboost"
      ],
      "metadata": {
        "id": "QgM1rvbEY5ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Datos de ejemplo: tamaño de la casa y número de habitaciones\n",
        "X = np.array([[1000, 2], [1500, 3], [2000, 4], [2500, 5]])\n",
        "# Precios correspondientes de las casas\n",
        "y = np.array([50000, 75000, 100000, 125000])\n",
        "\n",
        "# Crear el modelo de CatBoost Regressor\n",
        "model = CatBoostRegressor(iterations=50, learning_rate=0.1, depth=3)\n",
        "# Entrenar el modelo\n",
        "model.fit(X, y)\n",
        "\n",
        "# Datos de una nueva casa para predecir su precio\n",
        "new_house = np.array([[1800, 3]])\n",
        "# Predecir el precio de la nueva casa\n",
        "predicted_price = model.predict(new_house)\n",
        "# Imprimir el precio predicho\n",
        "print(\"El precio predicho de la nueva casa es:\", predicted_price)"
      ],
      "metadata": {
        "id": "v84j66srX7DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "JUfejCfvbvld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vemos otro ejemplo similar, pero en esta ocasión predeciremos el precio de las acciones de una empresa basándonos en algunas características financieras."
      ],
      "metadata": {
        "id": "AnwKG7lnb2rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from catboost import CatBoostRegressor\n",
        "\n",
        "# Datos de ejemplo: características financieras y precios de las acciones\n",
        "X = np.array([[100, 10], [150, 12], [200, 15], [250, 18]])  # Características financieras (por ejemplo, ingresos, gastos)\n",
        "y = np.array([50, 55, 60, 65])  # Precios correspondientes de las acciones\n",
        "\n",
        "# Crear el modelo de CatBoost Regressor\n",
        "model = CatBoostRegressor(iterations=30, learning_rate=0.1, depth=3)\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X, y)\n",
        "\n",
        "# Datos de una nueva empresa para predecir el precio de sus acciones\n",
        "new_company = np.array([[180, 13]])  # Nuevas características financieras\n",
        "\n",
        "# Predecir el precio de las acciones de la nueva empresa\n",
        "predicted_price = model.predict(new_company)\n",
        "\n",
        "# Imprimir el precio predicho\n",
        "print(\"El precio predicho de las acciones de la nueva empresa es:\", predicted_price)"
      ],
      "metadata": {
        "id": "Z_HZ1NKgcE6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este ejemplo es básicamente similar al anterior, con diferencia en las características, en este caso las características de ejemplo es ingresos y gastos.\n",
        "X contiene las características financieras de varias empresas(ingresos y los gastos), e Y contiene los precios correspondientes de las acciones. Continuamos con la creación del modelo para después utilizar los datos para el entrenamiento del modelo. Y al final imprimir las interaciones y el precio predicho final.\n"
      ],
      "metadata": {
        "id": "bdDJl6EqcQA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "4gKhr4GfcnZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuemos con un ejemplo más elaborado donde utilizaremos CatBoost Regressor para predecir el precio de diamantes utilizando un conjunto de datos realista."
      ],
      "metadata": {
        "id": "Ky-lg-a1crGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de precios de diamantes\n",
        "data = pd.read_csv(r\"c:\\Users\\CesarIvan\\OneDrive\\Escritorio\\diamonds.csv\")\n",
        "\n",
        "# Dividir los datos en características (X) y precios (y)\n",
        "X = data.drop(\"price\", axis=1)\n",
        "y = data[\"price\"]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Crear el modelo de CatBoost Regressor\n",
        "model = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=6, verbose=100)\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
        "\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio en el conjunto de prueba:\", mse)"
      ],
      "metadata": {
        "id": "GFnECLOBcx94",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "06i6U9kdu-9r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente ejemplo usaremos el mismo dataframe, pero en este ejemplo añadiremos más funciones de CatBoostRegressor y realizara una validación cruzada para evaluar el rendimiento del modelo."
      ],
      "metadata": {
        "id": "DO4EgCzVvjT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos\n",
        "data = pd.read_csv(r\"c:\\Users\\CesarIvan\\OneDrive\\Escritorio\\diamonds.csv\")\n",
        "# Dividir los datos en características (X) y precios (y)\n",
        "X = data.drop(\"price\", axis=1)\n",
        "y = data[\"price\"]\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Crear el modelo de CatBoost Regressor\n",
        "model = CatBoostRegressor(iterations=500, learning_rate=0.1, depth=6, verbose=100)\n",
        "\n",
        "# Realizar validación cruzada para evaluar el rendimiento del modelo\n",
        "cv_scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Entrenar el modelo en el conjunto de entrenamiento\n",
        "model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
        "# Realizar predicciones en el conjunto de prueba\n",
        "y_pred = model.predict(X_test)\n",
        "# Calcular el error cuadrático medio\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio en el conjunto de prueba:\", mse)\n",
        "\n",
        "# Imprimir los resultados de la validación cruzada\n",
        "print(\"Resultados de la validación cruzada:\")\n",
        "for i, score in enumerate(cv_scores):\n",
        "    print(f\"Fold {i+1}: {-score}\")\n",
        "print(f\"Promedio: {-np.mean(cv_scores)}\")\n"
      ],
      "metadata": {
        "id": "dWjRmTXfvw6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Kernel Ridge Regression (KRR)"
      ],
      "metadata": {
        "id": "Riyb4-SGzVAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuemos con los ejemplos."
      ],
      "metadata": {
        "id": "3RlIPymq2Wzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "YHXFl6gR2qY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como primer ejemplo veamos cómo se implementa KRR en Python."
      ],
      "metadata": {
        "id": "Q3y5JXNw2uxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "# Generar datos de ejemplo\n",
        "np.random.seed(0)\n",
        "X = 5 * np.random.rand(100, 1)  # Características (en este caso, solo una característica)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, 100)  # Variable objetivo (respuesta)\n",
        "\n",
        "# Instanciar el modelo Kernel Ridge Regression\n",
        "model = KernelRidge(kernel='rbf', gamma=0.1)\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "model.fit(X, y)\n",
        "\n",
        "# Generar datos de prueba para la visualización\n",
        "X_test = np.linspace(0, 5, 100)[:, np.newaxis]\n",
        "# Realizar predicciones usando el modelo entrenado\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.scatter(X, y, color='blue', label='Datos de entrenamiento')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicción')\n",
        "plt.xlabel('Característica')\n",
        "plt.ylabel('Variable Objetivo')\n",
        "plt.title('Predicción utilizando Kernel Ridge Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W9qbQoL-3Azn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "ASAw1GwS8vqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el siguiente código creamos un modelo de KRR utilizando un kernel polinomial de grado 2 y lo ajusta a los datos de entrenamiento (para este ejemplo usaremos de ejemplos kilometraje y precios de venta de coches usados). Luego, realiza predicciones para diferentes valores de kilometraje y visualiza los resultados. Para recordar que en una aplicación real, se podrían usar más características y ajustar los hiperparámetros del modelo para obtener resultados más precisos.\n",
        "\n"
      ],
      "metadata": {
        "id": "eK0SIJIf81fM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos de ejemplo: características y precios de algunos coches usados\n",
        "# Aquí, usaremos solo una característica (kilometraje) para simplificar\n",
        "X = np.array([[5000], [10000], [15000], [20000], [25000]])\n",
        "y = np.array([15000, 12000, 10000, 8000, 6000])\n",
        "\n",
        "# Instanciar el modelo Kernel Ridge Regression\n",
        "# Aquí, usaremos un kernel polinomial de grado 2 para mostrar un ejemplo simple\n",
        "model = KernelRidge(kernel='polynomial', degree=2)\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "model.fit(X, y)\n",
        "\n",
        "# Generar datos de prueba para la visualización\n",
        "X_test = np.linspace(0, 30000, 100)[:, np.newaxis]\n",
        "# Realizar predicciones usando el modelo entrenado\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.scatter(X, y, color='blue', label='Datos de entrenamiento')\n",
        "plt.plot(X_test, y_pred, color='red', linewidth=2, label='Predicción')\n",
        "plt.xlabel('Kilometraje')\n",
        "plt.ylabel('Precio de venta')\n",
        "plt.title('Predicción de precio de venta de coches usados')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BLJ9OMj48-BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3\n"
      ],
      "metadata": {
        "id": "Vpdg-cMa-qkm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el siguiente ejemplo subamos un poco la complejidad.\n",
        "Lo que haremos en este código es generar datos de ejemplo que representan características de productos (tamaño y peso) y sus precios correspondientes. Luego, con el modelo de Kernel Ridge Regression con un kernel polinomial de grado 3 y un coeficiente de regularización de 0.1. Ajusta el modelo a los datos de entrenamiento y realizamos predicciones sobre nuevos productos. Finalmente, graficamos los datos de entrenamiento y las predicciones del modelo para visualizar cómo se ajusta a los datos."
      ],
      "metadata": {
        "id": "hswjlZzi-u5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generar datos de ejemplo: características y precios de algunos productos\n",
        "# En este ejemplo, consideraremos dos características: tamaño y peso\n",
        "X = np.array([[2, 3], [3, 4], [4, 5], [5, 6], [6, 7]])\n",
        "y = np.array([20, 25, 30, 35, 40])\n",
        "\n",
        "# Instanciar el modelo Kernel Ridge Regression con kernel polinomial\n",
        "model = KernelRidge(kernel='polynomial', degree=3, alpha=0.1)\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "model.fit(X, y)\n",
        "\n",
        "# Generar datos de prueba para la visualización\n",
        "X_test = np.array([[3.5, 4.5], [4.5, 5.5], [5.5, 6.5]])  # Ejemplos de nuevos productos\n",
        "# Realizar predicciones usando el modelo entrenado\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Imprimir las predicciones\n",
        "print(\"Predicciones para nuevos productos:\")\n",
        "for i in range(len(X_test)):\n",
        "    print(\"Características:\", X_test[i], \" Predicción de precio:\", y_pred[i])\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.scatter(X[:, 0], y, color='blue', label='Datos de entrenamiento')\n",
        "plt.plot(X_test[:, 0], y_pred, color='red', marker='o', linestyle='-', label='Predicción')\n",
        "plt.xlabel('Tamaño')\n",
        "plt.ylabel('Precio')\n",
        "plt.title('Predicción de precios de productos utilizando Kernel Ridge Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fTVPkA1A-1Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "Rx5VRZv5Be5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo más complejo de cómo utilizar Kernel Ridge Regression en un problema más desafiante. Supongamos que trabajamos en un proyecto de análisis financiero y se busca predecir los precios de las acciones de una empresa basándonos en múltiples factores económicos y de mercado. Utilizaremos datos históricos para entrenar nuestro modelo y luego intentaremos predecir los precios futuros de las acciones."
      ],
      "metadata": {
        "id": "KQb5VHNUBk4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Cargar los datos históricos de precios de las acciones\n",
        "# Supongamos que tenemos datos de múltiples características como volumen, precio de cierre, etc.\n",
        "data = pd.read_csv(r\"c:\\Users\\CesarIvan\\OneDrive\\Escritorio\\HistoricalData.csv\")\n",
        "# Seleccionar características y variable objetivo\n",
        "X = data[['Volume','Open', 'High', 'Low']].values\n",
        "y = data['Close/Last'].values\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Escalar los datos\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Instanciar el modelo Kernel Ridge Regression con kernel RBF\n",
        "model = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\n",
        "\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "model.fit(X_train_scaled, y_train)\n",
        "# Realizar predicciones sobre los datos de prueba\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "# Calcular el error cuadrático medio (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Error cuadrático medio (MSE):\", mse)\n",
        "\n",
        "# Visualizar las predicciones\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_test, label='Precio real')\n",
        "plt.plot(y_pred, label='Predicción')\n",
        "plt.title('Predicción de precios de acciones utilizando Kernel Ridge Regression')\n",
        "plt.xlabel('Índice del dato de prueba')\n",
        "plt.ylabel('Precio')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "NO7Rzj3lBueI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression."
      ],
      "metadata": {
        "id": "6nIR0ip6DVhx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "GuHoVABNFzpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empecemos con un ejemplo básico de cómo implementar una regresión lineal en Python utilizando la biblioteca scikit-learn."
      ],
      "metadata": {
        "id": "0W3meKuuF4WV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos de ejemplo\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)  # Variables independientes\n",
        "y = np.array([2, 3, 4, 5, 6])  # Variable dependiente\n",
        "\n",
        "# Crear el modelo de regresión lineal\n",
        "modelo = LinearRegression()\n",
        "\n",
        "# Entrenar el modelo\n",
        "modelo.fit(X, y)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred = modelo.predict(X)\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.scatter(X, y, color='blue')  # Graficar los puntos de datos\n",
        "plt.plot(X, y_pred, color='red')  # Graficar la línea de regresión\n",
        "plt.xlabel('Variable independiente')\n",
        "plt.ylabel('Variable dependiente')\n",
        "plt.title('Regresión lineal simple')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yl6iU2r8F7-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo, X son las variables independientes (en este caso, solo una variable) e Y es la variable dependiente. **Creamos** un objeto LinearRegression de scikit-learn, lo **entrenamos** con nuestros datos utilizando el método fit(), y luego hacemos **predicciones** con el método predict().\n",
        "\n",
        "Finalmente, **visualizamos los resultados** trazando los puntos de datos y la línea de regresión en un gráfico utilizando matplotlib.\n"
      ],
      "metadata": {
        "id": "WtkSEaqsGIHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "eYwIUEN_GMjr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el siguiente ejemplo aplicaremos una regresión lineal múltiple."
      ],
      "metadata": {
        "id": "LddSi852GRAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Datos de ejemplo: supongamos que tenemos dos variables independientes X1 y X2,\n",
        "# y queremos predecir una variable dependiente Y.\n",
        "X = np.array([[1, 2], [2, 4], [3, 6], [4, 8]])  # Variables independientes\n",
        "Y = np.array([2, 4, 6, 8])  # Variable dependiente\n",
        "\n",
        "# Crear un objeto de regresión lineal\n",
        "modelo = LinearRegression()\n",
        "# Ajustar el modelo a los datos\n",
        "modelo.fit(X, Y)\n",
        "\n",
        "# Imprimir los coeficientes y la ordenada al origen\n",
        "print(\"Coeficientes:\", modelo.coef_)\n",
        "print(\"Ordenada al origen:\", modelo.intercept_)\n",
        "\n",
        "# Predecir nuevos valores\n",
        "nuevos_valores = np.array([[5, 10], [6, 12]])  # Nuevos valores de X\n",
        "predicciones = modelo.predict(nuevos_valores)\n",
        "print(\"Predicciones:\", predicciones)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.scatter(X[:, 0], Y, color='blue', label='Datos reales')  # Graficar X1 vs Y\n",
        "plt.scatter(X[:, 1], Y, color='red', label='Datos reales')  # Graficar X2 vs Y\n",
        "plt.plot(X[:, 0], modelo.predict(X), color='green', label='Regresión lineal X1')  # Graficar la regresión lineal para X1\n",
        "plt.plot(X[:, 1], modelo.predict(X), color='orange', label='Regresión lineal X2')  # Graficar la regresión lineal para X2\n",
        "plt.xlabel('Variables independientes')\n",
        "plt.ylabel('Variable dependiente')\n",
        "plt.title('Regresión lineal múltiple')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5X53xPHtGVBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este código tenemos dos variables independientes X1 y X2, y una variable dependiente Y. **Creamos** un objeto de regresión lineal, **ajustamos el modelo** a los datos utilizando el método fit, e **imprimimos los coeficientes** y la **ordenada al origen**. Luego, utilizamos el modelo entrenado para hacer **predicciones** sobre nuevos valores de X usando el método predict.\n",
        "\n"
      ],
      "metadata": {
        "id": "Un_F-nchGz0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "Xg6wbBAzHY4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subamos la complejidad."
      ],
      "metadata": {
        "id": "7naSc-A8Hb4R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generar datos sintéticos\n",
        "np.random.seed(0)\n",
        "X = 6 * np.random.rand(100, 3) - 3  # Tres características\n",
        "y = 0.5 * X[:, 0]**3 + X[:, 1]**2 - 2 * X[:, 2] + np.random.randn(100)\n",
        "\n",
        "# Aplicar transformación polinomial a las características\n",
        "poly_features = PolynomialFeatures(degree=3, include_bias=False)\n",
        "X_poly = poly_features.fit_transform(X)\n",
        "\n",
        "# Crear el modelo de regresión lineal\n",
        "modelo = LinearRegression()\n",
        "\n",
        "# Entrenar el modelo\n",
        "modelo.fit(X_poly, y)\n",
        "\n",
        "# Realizar predicciones sobre los datos de entrenamiento\n",
        "y_pred = modelo.predict(X_poly)\n",
        "# Calcular métricas de evaluación\n",
        "mse = mean_squared_error(y, y_pred)\n",
        "r2 = r2_score(y, y_pred)\n",
        "print(f'MSE (Mean Squared Error): {mse}')\n",
        "print(f'R^2 Score: {r2}')\n",
        "\n",
        "# Graficar los datos y la curva ajustada\n",
        "plt.scatter(X[:, 0], y, label='Datos')\n",
        "plt.plot(X[:, 0], y_pred, color='red', label='Curva Ajustada')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('y')\n",
        "plt.title('Regresión Lineal con Polinomio de Grado 3')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zQKN4CX8Hfmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "mhcalZR9I72j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Otro ejemplo de regresión lineal múltiple utilizando datos hipotéticos sobre el rendimiento académico de los estudiantes en función de diferentes variables."
      ],
      "metadata": {
        "id": "PQFgh0MNI-0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Crear un DataFrame de ejemplo con datos de estudiantes\n",
        "data = {\n",
        "    'Horas de estudio': [5, 3, 7, 2, 6],\n",
        "    'Horas de sueño': [8, 7, 6, 9, 7],\n",
        "    'Asistencia (%)': [90, 95, 80, 85, 88],\n",
        "    'Extracurriculares': ['Sí', 'No', 'Sí', 'Sí', 'No'],\n",
        "    'Calificación': [85, 70, 90, 65, 80]\n",
        "}\n",
        "\n",
        "# Convertir el diccionario en un DataFrame de pandas\n",
        "df = pd.DataFrame(data)\n",
        "# Convertir variables categóricas en variables dummy\n",
        "df = pd.get_dummies(df, columns=['Extracurriculares'])\n",
        "\n",
        "# Seleccionar las variables independientes y la variable dependiente(calificación)\n",
        "X = df.drop('Calificación', axis=1)\n",
        "Y = df['Calificación']\n",
        "\n",
        "# Dividir el conjunto de datos en datos de entrenamiento y datos de prueba\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "# Crear un objeto de regresión lineal\n",
        "modelo = LinearRegression()\n",
        "# Ajustar el modelo a los datos de entrenamiento\n",
        "modelo.fit(X_train, Y_train)\n",
        "\n",
        "# Imprimir los coeficientes y la ordenada al origen\n",
        "print(\"Coeficientes:\", modelo.coef_)\n",
        "print(\"Ordenada al origen:\", modelo.intercept_)\n",
        "# Hacer predicciones sobre los datos de prueba\n",
        "predicciones = modelo.predict(X_test)\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.scatter(Y_test, predicciones)\n",
        "plt.xlabel(\"Calificaciones reales\")\n",
        "plt.ylabel(\"Predicciones\")\n",
        "plt.title(\"Calificaciones reales vs Predicciones\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WQXu4WWZJFBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# XGBoost Regressor"
      ],
      "metadata": {
        "id": "oKPBP6--J9D5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "sbXKoIEXZrkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empecemos con un ejemplo simple de como implementar XGBoost regressor."
      ],
      "metadata": {
        "id": "2GAYFSDhZu3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos aleatorios para simular precios de casas\n",
        "np.random.seed(42)\n",
        "# Generar características (habitaciones, baños, metros cuadrados, edad de la casa)\n",
        "X = np.random.rand(100, 4)\n",
        "# Generar precios de casas como una combinación lineal de características con algo de ruido\n",
        "y = 2*X[:,0] + 3*X[:,1] + 4*X[:,2] + 5*X[:,3] + np.random.randn(100)\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Configurar el modelo XGBoost Regressor\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,\n",
        "                max_depth = 5, alpha = 10, n_estimators = 10)\n",
        "\n",
        "# Entrenar el modelo\n",
        "xg_reg.fit(X_train, y_train)\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Imprimir las predicciones\n",
        "print(\"Predicciones:\")\n",
        "for i, pred in enumerate(preds):\n",
        "    print(f\"Predicción {i+1}: {pred}\")\n",
        "# Calcular el error cuadrático medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"Error cuadrático medio:\", rmse)\n"
      ],
      "metadata": {
        "id": "m-zsFUC1ZyoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "PNxJZaOlatfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este ejemplo, usaremos el conjunto de datos de diabetes de Scikit-learn, que es un conjunto de datos un poco más complejo."
      ],
      "metadata": {
        "id": "yVgqrBEUay8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Configurar el modelo XGBoost Regressor con diferentes parámetros\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.5, learning_rate = 0.1,\n",
        "                max_depth = 8, alpha = 3, n_estimators = 200)\n",
        "\n",
        "# Entrenar el modelo\n",
        "xg_reg.fit(X_train, y_train)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Imprimir las predicciones\n",
        "print(\"Predicciones:\")\n",
        "for i, pred in enumerate(preds):\n",
        "    print(f\"Predicción {i+1}: {pred}\")\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"Error cuadrático medio:\", rmse)"
      ],
      "metadata": {
        "id": "B7x0pkrPa6VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "-nUrKivZbb1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Continuamos con un ejemplo más complejo, este incluye una búsqueda de hiperparámetros utilizando la técnica de búsqueda en cuadrícula (grid search) junto con una validación cruzada."
      ],
      "metadata": {
        "id": "U7TUrWrHbe8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de diabetes\n",
        "diabetes = load_diabetes()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir los parámetros para la búsqueda en cuadrícula\n",
        "param_grid = {\n",
        "    'learning_rate': [0.05, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
        "    'alpha': [1, 3, 5]\n",
        "}\n",
        "\n",
        "# Configurar el modelo XGBoost Regressor\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
        "\n",
        "# Realizar la búsqueda en cuadrícula con validación cruzada\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=cv, scoring='neg_mean_squared_error', verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "# Hacer predicciones en el conjunto de prueba con el mejor modelo\n",
        "preds = best_model.predict(X_test)\n",
        "\n",
        "# Imprimir las predicciones\n",
        "print(\"Predicciones:\")\n",
        "for i, pred in enumerate(preds):\n",
        "    print(f\"Predicción {i+1}: {pred}\")\n",
        "# Calcular el error cuadrático medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"Error cuadrático medio:\", rmse)\n",
        "# Imprimir los mejores parámetros encontrados\n",
        "print(\"Mejores parámetros encontrados:\")\n",
        "print(grid_search.best_params_)\n"
      ],
      "metadata": {
        "id": "CHbwNhApbiF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "jvgxobY8dItG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos otro ejemplo más complejo, utilizando un conjunto de datos diferente y ajustando más hiperparámetros."
      ],
      "metadata": {
        "id": "e9A_yYWJdL4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Cargar el conjunto de datos de California housing\n",
        "california_housing = fetch_california_housing()\n",
        "X, y = california_housing.data, california_housing.target\n",
        "\n",
        "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir los hiperparámetros para la búsqueda aleatoria\n",
        "param_dist = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7, 9],\n",
        "    'n_estimators': [50, 100, 150, 200],\n",
        "    'subsample': [0.5, 0.7, 0.9],\n",
        "    'colsample_bytree': [0.5, 0.7, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'reg_alpha': [0, 0.1, 0.5, 1],\n",
        "    'reg_lambda': [0, 0.1, 0.5, 1]\n",
        "}\n",
        "\n",
        "# Configurar el modelo XGBoost Regressor\n",
        "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
        "# Realizar la búsqueda aleatoria con validación cruzada\n",
        "random_search = RandomizedSearchCV(estimator=xg_reg, param_distributions=param_dist, n_iter=50, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtener el mejor modelo\n",
        "best_model = random_search.best_estimator_\n",
        "# Hacer predicciones en el conjunto de prueba con el mejor modelo\n",
        "preds = best_model.predict(X_test)\n",
        "\n",
        "# Calcular el error cuadrático medio\n",
        "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
        "print(\"Error cuadrático medio:\", rmse)\n",
        "# Imprimir los mejores parámetros encontrados\n",
        "print(\"Mejores parámetros encontrados:\")\n",
        "print(random_search.best_params_)\n"
      ],
      "metadata": {
        "id": "oWY3JXRudOc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LGBM Regressor"
      ],
      "metadata": {
        "id": "pepp_gZieB8D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 1"
      ],
      "metadata": {
        "id": "050Qsb6Xe-0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar las bibliotecas necesarias\n",
        "import numpy as np\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Datos de ejemplo (características y etiquetas)\n",
        "X_train = np.array([[1, 2, 3],\n",
        "                    [4, 5, 6],\n",
        "                    [7, 8, 9]])\n",
        "y_train = np.array([10, 20, 30])\n",
        "\n",
        "# Crear y entrenar el modelo LGBM Regressor\n",
        "model = LGBMRegressor()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Hacer una predicción\n",
        "X_test = np.array([[2, 3, 4]])  # Nuevas características para predecir\n",
        "prediction = model.predict(X_test)\n",
        "\n",
        "print(\"Predicción:\", prediction)"
      ],
      "metadata": {
        "id": "5HoLN86ofFs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este ejemplo, primero **importamos** la clase LGBMRegressor del paquete lightgbm. Luego se **crea un conjunto de datos** de ejemplo con algunas características (X_train) y etiquetas correspondientes (y_train). Para continuar, se **crea una instancia del modelo LGBMRegressor** y lo entrenamos con los datos de entrenamiento. Después, **creamos un conjunto de características** de prueba (X_test) y utilizamos el modelo entrenado para **hacer una predicción** sobre estos datos. Finalmente, **imprimimos la predicción** resultante."
      ],
      "metadata": {
        "id": "kpkUv2vmfB8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 2"
      ],
      "metadata": {
        "id": "0YGGKQkMfqmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Generar datos sintéticos de regresión\n",
        "X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Configurar el modelo LGBMRegressor\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': ['l2', 'l1'],  # Cambiar el conjunto por una lista\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Crear y entrenar el modelo LGBMRegressor\n",
        "gbm = lgb.LGBMRegressor(**params)\n",
        "gbm.fit(X_train, y_train)\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred = gbm.predict(X_test)\n",
        "\n",
        "# Evaluar el rendimiento del modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)"
      ],
      "metadata": {
        "id": "xTWlf9Nff0Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 3"
      ],
      "metadata": {
        "id": "OPbsIz72g9jR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para el siguiente ejemplo se sigue un procedimiento similar al anterior, pero utilizando un conjunto de datos local(train.csv) en lugar de utilizar datos sintéticos."
      ],
      "metadata": {
        "id": "gVi6JGJMhBHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Cargar el conjunto de datos de viviendas\n",
        "df = pd.read_csv(r\"c:\\Users\\----\\Escritorio\\train.csv\")\n",
        "# Seleccionar características y etiquetas\n",
        "X = df.drop(columns=[\"SalePrice\"])\n",
        "y = df[\"SalePrice\"]\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Definir los parámetros para el modelo LGBMRegressor\n",
        "params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'objective': 'regression',\n",
        "    'metric': 'l2',\n",
        "    'num_leaves': 31,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.9,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'verbose': 0\n",
        "}\n",
        "\n",
        "# Crear y entrenar el modelo LGBMRegressor\n",
        "gbm = lgb.LGBMRegressor(**params)\n",
        "gbm.fit(X_train, y_train)\n",
        "# Predecir en el conjunto de prueba\n",
        "y_pred = gbm.predict(X_test)\n",
        "# Evaluar el rendimiento del modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)\n"
      ],
      "metadata": {
        "id": "YDmCJFdMhExm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de **importar las librerias**, cargamos el **conjunto de datos locales**, para después **seleccionar las características y etiquetas**.\n",
        "\n",
        "Continuamos con la **división el conjunto de datos en train y test**, y seguimos con la **definición de los parámetros para el modelo**.\n",
        "\n",
        "Para ultimo **crear y entrenar nuestro modelo**, se realizan las **predicciones** y evaluamos el **rendimiento del modelo**.\n"
      ],
      "metadata": {
        "id": "8crDSb9bhTpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejemplo 4"
      ],
      "metadata": {
        "id": "d_sUgvrGhztX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos un ejemplo más completo y complejo que aborda un problema de regresión utilizando el conjunto de datos de precios de viviendas de California (California Housing Prices dataset).\n",
        "\n",
        "En este ejemplo, además de la limpieza y preprocesamiento de datos, también utilizaremos la técnica de validación cruzada y realizaremos una evaluación más exhaustiva del modelo.\n"
      ],
      "metadata": {
        "id": "2ffygsW0h4-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Cargar el conjunto de datos de precios de viviendas de California\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Convertir a DataFrame de pandas para mayor comodidad\n",
        "df = pd.DataFrame(data=X, columns=california_housing.feature_names)\n",
        "df['target'] = y\n",
        "\n",
        "# Seleccionar características y etiquetas\n",
        "X = df.drop(columns=[\"target\"])\n",
        "y = df[\"target\"]\n",
        "\n",
        "# Dividir el conjunto de datos en entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Definir la cuadrícula de hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'boosting_type': ['gbdt', 'dart'],\n",
        "    'num_leaves': [20, 30, 40],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    'colsample_bytree': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Crear el modelo LGBMRegressor\n",
        "gbm = lgb.LGBMRegressor()\n",
        "\n",
        "# Realizar la búsqueda de cuadrícula utilizando validación cruzada\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "grid_search = GridSearchCV(gbm, param_grid, cv=kf, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "best_gbm = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba utilizando el mejor modelo\n",
        "y_pred = best_gbm.predict(X_test)\n",
        "\n",
        "# Evaluar el rendimiento del mejor modelo\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print('Mean Squared Error:', mse)\n",
        "print('Best Parameters:', best_params)\n"
      ],
      "metadata": {
        "id": "x76AfIMjh8_W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}